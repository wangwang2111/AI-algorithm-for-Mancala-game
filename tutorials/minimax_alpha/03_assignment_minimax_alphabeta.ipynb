{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b84146",
   "metadata": {},
   "source": [
    "\n",
    "# Mancala Search Assignment (No Solutions) — Minimax, Alpha–Beta, Ordering, Simulation, Custom Heuristic\n",
    "\n",
    "This is the **assignment-only** version. Cells contain **stubs** and **TODOs** for you to complete.\n",
    "\n",
    "## Tasks\n",
    "1. Implement `score_for(state, root_idx)`.\n",
    "2. Implement **Minimax** (no pruning) with correct **extra-turn depth** handling.\n",
    "3. Implement **Alpha–Beta** with the same extra-turn rule.\n",
    "4. Implement **one-ply move ordering** (`order_moves_by_one_ply_score`) and compare **node visits**.\n",
    "5. Run **100-game simulations** (Minimax vs Random; Alpha–Beta vs Random with/without ordering) and report:\n",
    "   - win_rate, avg time per move, avg nodes per move, avg moves per game.\n",
    "6. Implement your **own heuristic** and a **Minimax variant** that uses it; test vs Random.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c527538",
   "metadata": {},
   "source": [
    "## 0) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e11aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your repo's src/ to the Python path and import the engine.\n",
    "import sys, pathlib\n",
    "REPO_ROOT = pathlib.Path().resolve()\n",
    "SRC = REPO_ROOT / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "# Engine API expected:\n",
    "#   new_game(), legal_actions(state), step(state, action), evaluate(state)\n",
    "from mancala_ai.engine.core import new_game, legal_actions, step, evaluate\n",
    "\n",
    "print(\"Engine imports OK from:\", SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9aa36",
   "metadata": {},
   "source": [
    "## 1) Utilities and Stubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, time, random\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "\n",
    "class Stats:\n",
    "    \"\"\"Tracks node expansions during search.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.visits = 0\n",
    "\n",
    "def pretty(state: Dict):\n",
    "    \"\"\"Optional: quick text view of the board.\"\"\"\n",
    "    p0, p1 = state[\"pits\"][0], state[\"pits\"][1]\n",
    "    st0, st1 = state[\"stores\"]\n",
    "    print(\"+----- Mancala -----+\")\n",
    "    print(\"P1 store:\", st1)\n",
    "    print(\"P1 pits: \", list(reversed(p1)))\n",
    "    print(\"P0 pits: \", p0)\n",
    "    print(\"P0 store:\", st0)\n",
    "    print(\"Turn: P\", state[\"current_player\"], sep=\"\")\n",
    "    print(\"+-------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c0c34",
   "metadata": {},
   "source": [
    "### TODO A — `score_for(state, root_idx)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e83986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_for(state: Dict, root_idx: int) -> float:\n",
    "    \"\"\"Return a score for 'state' from the perspective of 'root_idx' (0 or 1).\n",
    "    HINT: Temporarily set state['current_player'] = root_idx before calling evaluate(state).\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement score_for(state, root_idx)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a10b3",
   "metadata": {},
   "source": [
    "## 2) Minimax (no pruning) — **Implement me**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(state: Dict, depth: int, root_idx: int, stats: Stats) -> Tuple[float, Optional[int]]:\n",
    "    \"\"\"Plain Minimax on the state-based Mancala engine.\n",
    "\n",
    "    Requirements:\n",
    "    - Terminal/leaf: depth==0 or one side's pits sum to 0 → return score_for(...), None\n",
    "    - Generate actions: legal_actions(state)\n",
    "    - Extra-turn rule: after step(state, a) → next_state,\n",
    "        if next_state['current_player'] == state['current_player'],\n",
    "        DO NOT decrement depth for that recursion (same ply).\n",
    "    - If it's root_idx's turn → maximize; else minimize.\n",
    "    - Increment stats.visits at each call.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement minimax(...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beeb45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move_minimax(state: Dict, depth: int = 5) -> Tuple[int, Stats]:\n",
    "    \"\"\"Return (move, stats) using your minimax implementation.\n",
    "    Fallback: if no move (None), choose first legal action or 0.\n",
    "    \"\"\"\n",
    "    # TODO: call minimax(...) and handle fallbacks\n",
    "    raise NotImplementedError(\"Implement choose_move_minimax(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a7d66",
   "metadata": {},
   "source": [
    "## 3) Alpha–Beta (with optional ordering) — **Implement me**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphabeta(state: Dict, depth: int, alpha: float, beta: float, root_idx: int,\n",
    "              stats: Stats, ordering_fn: Optional[Callable[[Dict, int, List[int]], List[int]]] = None\n",
    "             ) -> Tuple[float, Optional[int]]:\n",
    "    \"\"\"Alpha–Beta pruning with the same extra-turn rule as Minimax.\n",
    "\n",
    "    Requirements:\n",
    "    - Apply ordering function if provided:\n",
    "        acts = ordering_fn(state, root_idx, acts) if ordering_fn else acts\n",
    "    - Use alpha/beta cuts where appropriate.\n",
    "    - Increment stats.visits at each call.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement alphabeta(...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move_alphabeta(state: Dict, depth: int = 7,\n",
    "                          ordering_fn: Optional[Callable[[Dict, int, List[int]], List[int]]] = None\n",
    "                         ) -> Tuple[int, Stats]:\n",
    "    \"\"\"Return (move, stats) using your alphabeta implementation.\"\"\"\n",
    "    # TODO: implement wrapper similar to choose_move_minimax\n",
    "    raise NotImplementedError(\"Implement choose_move_alphabeta(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00542e",
   "metadata": {},
   "source": [
    "## 4) Move Ordering — **Implement me**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038cd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_moves_by_one_ply_score(state: Dict, root_idx: int, acts: List[int]) -> List[int]:\n",
    "    \"\"\"Return actions sorted DESC by one-ply child score (use score_for on step(state, a)).\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError(\"Implement order_moves_by_one_ply_score(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb09dc6",
   "metadata": {},
   "source": [
    "## 5) Compare visits (with vs without ordering) — **Run after implementing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reproducible mid-game challenge state\n",
    "CHALLENGE_STATE = {\n",
    "    \"pits\":   [[0, 3, 0, 5, 1, 7], [4, 0, 6, 0, 2, 5]],\n",
    "    \"stores\": [5, 10],\n",
    "    \"current_player\": 0,\n",
    "}\n",
    "\n",
    "def compare_visits_on_state(state: Dict, depth: int = 8):\n",
    "    s1 = copy.deepcopy(state); s2 = copy.deepcopy(state)\n",
    "    mv_no, st_no   = choose_move_alphabeta(s1, depth=depth, ordering_fn=None)\n",
    "    mv_ord, st_ord = choose_move_alphabeta(s2, depth=depth, ordering_fn=order_moves_by_one_ply_score)\n",
    "    print(f\"Alpha–Beta d={depth} WITHOUT ordering: move={mv_no}, nodes={st_no.visits}\")\n",
    "    print(f\"Alpha–Beta d={depth} WITH    ordering: move={mv_ord}, nodes={st_ord.visits}\")\n",
    "\n",
    "# After you implement Alpha–Beta and ordering, uncomment to run:\n",
    "# compare_visits_on_state(CHALLENGE_STATE, depth=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11e66c",
   "metadata": {},
   "source": [
    "## 6) Simulation Harness (provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ca59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent():\n",
    "    def _fn(state: Dict):\n",
    "        acts = legal_actions(state)\n",
    "        mv = random.choice(acts) if acts else 0\n",
    "        st = Stats()\n",
    "        return mv, st\n",
    "    return _fn\n",
    "\n",
    "def minimax_agent(depth: int):\n",
    "    def _fn(state: Dict):\n",
    "        return choose_move_minimax(state, depth=depth)\n",
    "    return _fn\n",
    "\n",
    "def alphabeta_agent(depth: int, ordering: bool = False):\n",
    "    def _fn(state: Dict):\n",
    "        order = order_moves_by_one_ply_score if ordering else None\n",
    "        return choose_move_alphabeta(state, depth=depth, ordering_fn=order)\n",
    "    return _fn\n",
    "\n",
    "def play_game(agent0, agent1, max_plies=500):\n",
    "    s = new_game(); moves = 0\n",
    "    times_0, times_1, nodes_0, nodes_1 = [], [], [], []\n",
    "    while sum(s[\"pits\"][0]) > 0 and sum(s[\"pits\"][1]) > 0 and moves < max_plies:\n",
    "        turn = s[\"current_player\"]\n",
    "        agent = agent0 if turn == 0 else agent1\n",
    "        t0 = time.perf_counter()\n",
    "        mv, st = agent(copy.deepcopy(s))\n",
    "        dt = time.perf_counter() - t0\n",
    "        s, _, _ = step(s, mv)\n",
    "        moves += 1\n",
    "        if turn == 0:\n",
    "            times_0.append(dt); nodes_0.append(st.visits)\n",
    "        else:\n",
    "            times_1.append(dt); nodes_1.append(st.visits)\n",
    "    st0, st1 = s[\"stores\"]\n",
    "    winner = 0 if st0 > st1 else 1 if st1 > st0 else -1\n",
    "    return {\"winner\": winner, \"moves\": moves,\n",
    "            \"times_0\": times_0, \"times_1\": times_1,\n",
    "            \"nodes_0\": nodes_0, \"nodes_1\": nodes_1}\n",
    "\n",
    "def run_series(agentA, agentB, n_games=100, seed=42):\n",
    "    random.seed(seed)\n",
    "    wins_A = wins_B = draws = 0\n",
    "    moves_list = []; tA=[]; tB=[]; nA=[]; nB=[]\n",
    "    for g in range(n_games):\n",
    "        if g % 2 == 0:\n",
    "            res = play_game(agentA, agentB)\n",
    "            if   res[\"winner\"] == 0: wins_A += 1\n",
    "            elif res[\"winner\"] == 1: wins_B += 1\n",
    "            else: draws += 1\n",
    "            tA += res[\"times_0\"]; tB += res[\"times_1\"]\n",
    "            nA += res[\"nodes_0\"]; nB += res[\"nodes_1\"]\n",
    "        else:\n",
    "            res = play_game(agentB, agentA)\n",
    "            if   res[\"winner\"] == 0: wins_B += 1\n",
    "            elif res[\"winner\"] == 1: wins_A += 1\n",
    "            else: draws += 1\n",
    "            tA += res[\"times_1\"]; tB += res[\"times_0\"]\n",
    "            nA += res[\"nodes_1\"]; nB += res[\"nodes_0\"]\n",
    "        moves_list.append(res[\"moves\"])\n",
    "    def _avg(xs): return (sum(xs)/len(xs)) if xs else 0.0\n",
    "    return {\n",
    "        \"games\": n_games,\n",
    "        \"wins_A\": wins_A, \"wins_B\": wins_B, \"draws\": draws,\n",
    "        \"win_rate_A\": wins_A/n_games, \"win_rate_B\": wins_B/n_games,\n",
    "        \"avg_moves_per_game\": _avg(moves_list),\n",
    "        \"avg_time_per_move_A\": _avg(tA), \"avg_time_per_move_B\": _avg(tB),\n",
    "        \"avg_nodes_per_move_A\": _avg(nA), \"avg_nodes_per_move_B\": _avg(nB),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f05774",
   "metadata": {},
   "source": [
    "### 6.1) Run: Minimax vs Alpha-Beta (after you implement Minimax) and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71147c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment after implementing choose_move_minimax:\n",
    "# summary = run_series(minimax_agent(depth=5), alphabeta_agent(depth=5, ordering=False), n_games=100, seed=123)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f9cf1",
   "metadata": {},
   "source": [
    "## 7) Your Heuristic — **Implement me**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_score_for(state: Dict, root_idx: int) -> float:\n",
    "    \"\"\"Return a custom heuristic score from root_idx's perspective.\n",
    "    Replace this with your design (store diff, material, capture threats, etc.).\n",
    "    \"\"\"\n",
    "    # TODO: implement your heuristic\n",
    "    raise NotImplementedError(\"Implement heuristic_score_for(...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_with_advanced_heuristic(state: Dict, depth: int, root_idx: int, stats: Stats):\n",
    "    \"\"\"Minimax that uses heuristic_score_for at leaves/terminal instead of evaluate().\"\"\"\n",
    "    # TODO: implement (mirror your minimax, but call heuristic_score_for at leaves)\n",
    "    raise NotImplementedError(\"Implement minimax_with_heuristic(...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27375764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_move_minimax_heuristic(state: Dict, depth: int = 5):\n",
    "    # TODO: wrapper similar to choose_move_minimax but for your heuristic version\n",
    "    raise NotImplementedError(\"Implement choose_move_minimax_heuristic(...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eaae0",
   "metadata": {},
   "source": [
    "### 7.1) Run: Your Heuristic Minimax vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment after implementing your heuristic and wrapper:\n",
    "# summary = run_series(lambda s: choose_move_minimax_heuristic(s, depth=5),\n",
    "#                      random_agent(), n_games=100, seed=123)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3036e0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## What to submit\n",
    "1. **Code** for all TODOs.\n",
    "2. A brief **report** including:\n",
    "   - Visit comparison (Alpha–Beta w/ and w/o ordering) on `CHALLENGE_STATE`.\n",
    "   - 100-game metrics for each matchup.\n",
    "   - Description of your heuristic and its 100-game results vs Random.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
